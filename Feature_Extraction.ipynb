{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCgqJtFZHjcO5vjlzhOjxH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wkambale/Feature-Extraction-in-ML/blob/main/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Techniques of Feature Extraction in Machine Learning\n",
        "\n",
        "From Numerical Scaling to Image Descriptors: Mastering Feature Extraction for Optimal Machine Learning Models for Enhanced Model Performance\n",
        "\n",
        "Link to Article: https://kambale.dev//feature-extraction-in-ml\n",
        "\n",
        "NB: To use this notebook, make a copy first.\n",
        "\n",
        "MIT License: Copyright (c) 2023 **Wesley Kambale**"
      ],
      "metadata": {
        "id": "Dql8aOjknklq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numerical Feature Extraction"
      ],
      "metadata": {
        "id": "DydfMOI8n-SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = [[10, 0.5], [20, 0.8], [15, 0.7]]\n",
        "\n",
        "# Create an instance of the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply scaling to the data\n",
        "scaled_features = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "g48l5yMQnmn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning \n",
        "import numpy as np\n",
        "\n",
        "data = [2.5, 3.7, 1.9, 4.2, 5.1, 2.8]\n",
        "\n",
        "# Create four bins from 1 to 6\n",
        "bins = np.linspace(1, 6, 4) \n",
        "\n",
        "# Assign each value to a bin\n",
        "binned_features = np.digitize(data, bins)  "
      ],
      "metadata": {
        "id": "a8lxpVHmogZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregation\n",
        "import numpy as np\n",
        "\n",
        "data = [[10, 20, 15], [5, 10, 8], [12, 18, 20]]\n",
        "\n",
        "# Compute mean along each column (axis=0)\n",
        "mean_features = np.mean(data, axis=0)  \n",
        "\n",
        "# Compute median along each column\n",
        "median_features = np.median(data, axis=0)"
      ],
      "metadata": {
        "id": "BnXdGjm0opYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial Features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "data = [[2, 3], [1, 4], [5, 2]]\n",
        "\n",
        "# Create polynomial features up to degree 2\n",
        "poly = PolynomialFeatures(degree=2) \n",
        "\n",
        "# Generate polynomial features\n",
        "polynomial_features = poly.fit_transform(data)  "
      ],
      "metadata": {
        "id": "FfwMnte1oy6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Feature Extraction"
      ],
      "metadata": {
        "id": "ZRaVJgbVo-23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "data = [['Red'], ['Blue'], ['Green'], ['Red']]\n",
        "\n",
        "# Create an instance of the OneHotEncoder\n",
        "encoder = OneHotEncoder()  \n",
        "\n",
        "# Apply one-hot encoding\n",
        "onehot_features = encoder.fit_transform(data).toarray()  "
      ],
      "metadata": {
        "id": "Cm2rdIHcpAsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Low', 'High', 'Medium', 'Low']\n",
        "\n",
        " # Create an instance of the LabelEncoder\n",
        "encoder = LabelEncoder() \n",
        "\n",
        " # Apply label encoding\n",
        "encoded_features = encoder.fit_transform(data) "
      ],
      "metadata": {
        "id": "4xQAuHztpJ1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Encoding\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.Series(['Apple', 'Banana', 'Apple', 'Orange', 'Banana'])\n",
        "\n",
        "# Compute frequency of each category\n",
        "frequency = data.value_counts(normalize=True)  \n",
        "\n",
        "# Replace categories with frequencies\n",
        "encoded_features = data.map(frequency)  "
      ],
      "metadata": {
        "id": "PMVlfkawpSex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target Encoding\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Category': ['A', 'B', 'A', 'B'], 'Target': [1, 0, 1, 1]})\n",
        "\n",
        "# Compute mean target value for each category\n",
        "target_mean = data.groupby('Category')['Target'].mean() \n",
        "\n",
        "# Replace categories with mean target values\n",
        "encoded_features = data['Category'].map(target_mean)"
      ],
      "metadata": {
        "id": "yl3_2CeOpZew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Feature Extraction"
      ],
      "metadata": {
        "id": "532h36erpiCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words (BoW)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data = ['I love dogs', 'I hate cats', 'Dogs are cute']\n",
        "\n",
        " # Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer() \n",
        "\n",
        "# Apply BoW transformation\n",
        "bow_features = vectorizer.fit_transform(data) "
      ],
      "metadata": {
        "id": "GJFhvt8AplWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "data = ['I love dogs', 'I hate cats', 'Dogs are cute']\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()  \n",
        "\n",
        "# Apply TF-IDF transformation\n",
        "tfidf_features = vectorizer.fit_transform(data)  "
      ],
      "metadata": {
        "id": "k_ejrBt6ptzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "data = [['I', 'love', 'dogs'], ['I', 'hate', 'cats'], ['Dogs', 'are', 'cute']]\n",
        "\n",
        "# Create a Word2Vec model\n",
        "model = Word2Vec(data, min_count=1)  \n",
        "\n",
        "# Obtain the word embedding for 'dogs'\n",
        "word_embedding = model.wv['dogs']  "
      ],
      "metadata": {
        "id": "g0wx2G5ap0cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-grams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "data = ['I love dogs', 'I hate cats', 'Dogs are cute']\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "ngram_features = vectorizer.fit_transform(data)"
      ],
      "metadata": {
        "id": "krcId_JQp8lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Feature Extraction"
      ],
      "metadata": {
        "id": "35F44tTpqEu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of Oriented Gradients (HOG)\n",
        "import cv2\n",
        "\n",
        "image = cv2.imread('image.jpg', 0)\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog_features = hog.compute(image)"
      ],
      "metadata": {
        "id": "sElGpPvQqJXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale-Invariant Feature Transform (SIFT)\n",
        "import cv2\n",
        "\n",
        "image = cv2.imread('image.jpg', 0)\n",
        "sift = cv2.SIFT_create()\n",
        "keypoints, descriptors = sift.detectAndCompute(image, None)"
      ],
      "metadata": {
        "id": "dbldAP0aqVOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Networks (CNNs)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "image = tf.keras.preprocessing.image.load_img('image.jpg', target_size=(224, 224))\n",
        "image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False)\n",
        "features = vgg_model.predict(np.expand_dims(image, axis=0))"
      ],
      "metadata": {
        "id": "eapS1GsWqcbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained Models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "image = tf.keras.preprocessing.image.load_img('image.jpg', target_size=(224, 224))\n",
        "image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False)\n",
        "intermediate_layer_model = tf.keras.Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block4_pool').output)\n",
        "features = intermediate_layer_model.predict(np.expand_dims(image, axis=0))"
      ],
      "metadata": {
        "id": "YINWBeKfqjGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Feature Extraction Techniques"
      ],
      "metadata": {
        "id": "-uhzsJyrquUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming X is your input data\n",
        "\n",
        "# Specify the number of components you want to extract\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# X_pca contains the extracted features with reduced dimensionality\n",
        "X_pca = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "KrZbYUporCGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Independent Component Analysis (ICA)\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "# Assuming X is your input data\n",
        "\n",
        " # Specify the number of components you want to extract\n",
        "ica = FastICA(n_components=2)\n",
        "\n",
        "# X_ica contains the extracted features with reduced dimensionality\n",
        "X_ica = ica.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "RPW2LKCCrC6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Assuming X and y are your input features and target labels, respectively\n",
        "\n",
        " # Select the top 10 features\n",
        "selector = SelectKBest(score_func=chi2, k=10)\n",
        "\n",
        "# X_new contains the selected features\n",
        "X_new = selector.fit_transform(X, y)\n"
      ],
      "metadata": {
        "id": "BvE2961arN2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}